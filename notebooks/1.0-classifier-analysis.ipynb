{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Geron's text, we define various metrics to analyze classifiers and apply Bayes rule to measure the effectiveness of classifiers in practice.\n",
    "\n",
    "Also see Dr. Goldman's lecture on [logistic regression](https://github.com/doriang102/APM4990-/blob/master/lectures/Lecture%204%20-%20Classification%20.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "Once a treshold, say $0.5$, is chosen, we can define the precision and recall as follows. Let $P$ be the number of positives predicted by the classifier, $T$ be the number of true predictions, and $TP$ be the number positive predictions of the classifier that are also true.\n",
    "\n",
    "Define the recall, $R$, and precision, $P$, by:\n",
    "\n",
    "$R = \\frac{TP}{T} \\: \\:\\: \\:\\:\\:\\:\\:  P = \\frac{TP}{P}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score\n",
    "\n",
    "Once we fix a treshold, a good aggregate measure of a classifier is the $F_1$-score defined as follows. We define the $F_1$-score as the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of the precision and recall:\n",
    "\n",
    "\\begin{equation}\n",
    "    F_1 = \\frac{2}{\\frac{1}{P} + \\frac{1}{R}}\n",
    "\\end{equation}\n",
    "\n",
    "It has the following properties:\n",
    "\n",
    "$0 \\leq \\operatorname{min}(P, R) \\leq F_1 \\leq \\sqrt{P R} \\leq \\frac{P + R}{2} \\leq \\operatorname{max}(P, R) \\leq 1$\n",
    "\n",
    "with equality holding in the inner inequalities iff $P = R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC\n",
    "\n",
    "Despite being an aggregate measure of classifier quality, a clear downside of the $F_1$-score is that it depends on the threshold we chose, $0.5$. Besides inspecting the precision-recall curve, we can obtain another aggregate measure, independent of the treshold, using the ROC curve.\n",
    "\n",
    "The ROC curve is defined as the true positive rate (TPR also called recall) vs. the false positive rate, calculated for each treshold. An aggregate measure is then derived by calculating the area under the ROC curve, called the *ROC AUC*. See either notebook using BERT or Naive Bayes for calculations of this metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding the base rate fallacy\n",
    "\n",
    "Following Murphy's text on machine learning (section 2.2.3.1), we describe how to obtain accurate predictions from classifiers, with disaster tweets as our main example.\n",
    "\n",
    "Let $X$ be a Bernoulli random variable repesenting the classifiers prediction and $Y$ be a Bernoulli random variable repesenting whether or not a tweet actually corresponds to a disaster.\n",
    "\n",
    "Suppose the recall, i.e. $p(x = 1 | y = 1) = R$, is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $m = p(y = 1)$ be the proportion of tweets that correspond to disasters. A generous estimate will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0.004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this percentage is much lower than the proportion of disaster tweets in the dataset used to build our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The false positive rate, $p(x=1 | y=0)$ (one minus the recall of the negative class), is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = 1 - 0.88\n",
    "fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining these three terms by using bayes rule we can compute the correct answer as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(y = 1 | x = 1) = \\frac{p(x = 1 | y =1)p(y = 1)}{p(x = 1 | y = 1) p( y = 1) + p(x = 1 | y = 0) p( y = 0)}\n",
    "\\end{equation}\n",
    "\n",
    "Thus $p(y = 1 | x = 1)$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02480417754569191"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(R * m)/(R *m + fp *(1-m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be made higher by:\n",
    "\n",
    "\n",
    "(1) applying the classifier on tweets that are more likely to be disaster tweets,\n",
    "\n",
    "(2) decreasing the false positive rate of the classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
